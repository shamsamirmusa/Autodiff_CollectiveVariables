{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import jax.numpy as jnp\n",
    "from jax.scipy.linalg import eigh\n",
    "from jax import grad, jacobian, value_and_grad, jit, lax, vmap, value_and_grad\n",
    "import MDAnalysis as mda\n",
    "import re\n",
    "import jax\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract LJ parameters from topology file converted to txt.\n",
    "def extract_lj_parameters(path):\n",
    "    with open(path, 'r') as file:\n",
    "        data = file.read()\n",
    "    lj_sr_pattern = r\"type=(\\S+).*?c6=\\s*([0-9\\.\\-eE+]+).*?c12=\\s*([0-9\\.\\-eE+]+)\"\n",
    "    lj_sr_matches = re.findall(lj_sr_pattern, data)\n",
    "    lj_params = {atom_type: {\"c6\": float(c6), \"c12\": float(c12)} for atom_type, c6, c12 in lj_sr_matches}\n",
    "    return lj_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr_file = \"/Users/sss/Documents/EnergyGap_project/ENGgromax_output_files/md.tpr\"\n",
    "xtc_file = \"/Users/sss/Documents/EnergyGap_project/ENGgromax_output_files/md.xtc\"\n",
    "lj_contents = \"/Users/sss/Documents/EnergyGap_project/ENGgromax_output_files/tpr_contents.txt\"\n",
    "lj_params = extract_lj_parameters(lj_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residue 1, ASN: Processed Atoms - [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "Residue 2, LEU: Processed Atoms - [16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34]\n",
      "Residue 3, TYR: Processed Atoms - [35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55]\n",
      "Residue 4, ILE: Processed Atoms - [56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74]\n",
      "Residue 5, GLN: Processed Atoms - [75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91]\n",
      "Residue 6, TRP: Processed Atoms - [ 92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109\n",
      " 110 111 112 113 114 115]\n",
      "Residue 7, LEU: Processed Atoms - [116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133\n",
      " 134]\n",
      "Residue 8, LYS: Processed Atoms - [135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152\n",
      " 153 154 155 156]\n",
      "Residue 9, ASP: Processed Atoms - [157 158 159 160 161 162 163 164 165 166 167 168]\n",
      "Residue 10, GLY: Processed Atoms - [169 170 171 172 173 174 175]\n",
      "Residue 11, GLY: Processed Atoms - [176 177 178 179 180 181 182]\n",
      "Residue 12, PRO: Processed Atoms - [183 184 185 186 187 188 189 190 191 192 193 194 195 196]\n",
      "Residue 13, SER: Processed Atoms - [197 198 199 200 201 202 203 204 205 206 207]\n",
      "Residue 14, SER: Processed Atoms - [208 209 210 211 212 213 214 215 216 217 218]\n",
      "Residue 15, GLY: Processed Atoms - [219 220 221 222 223 224 225]\n",
      "Residue 16, ARG: Processed Atoms - [226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243\n",
      " 244 245 246 247 248 249]\n",
      "Residue 17, PRO: Processed Atoms - [250 251 252 253 254 255 256 257 258 259 260 261 262 263]\n",
      "Residue 18, PRO: Processed Atoms - [264 265 266 267 268 269 270 271 272 273 274 275 276 277]\n",
      "Residue 19, PRO: Processed Atoms - [278 279 280 281 282 283 284 285 286 287 288 289 290 291]\n",
      "Residue 20, SER: Processed Atoms - [292 293 294 295 296 297 298 299 300 301 302 303]\n"
     ]
    }
   ],
   "source": [
    "# #Introducing the system\n",
    "# u = mda.Universe(tpr_file, xtc_file )\n",
    "# print(f\"{u}, unlike in the paper it reports 39768 atoms\\n  However the protein atoms is {len(u.select_atoms('protein'))} same as the paper\" )\n",
    "# time_step = u.trajectory.dt \n",
    "# print(f\"The time step for the trajectories is : {time_step} ps\" )\n",
    "# print(f\"The system has {u.trajectory.n_frames} frames\")\n",
    "# print(f\"The total simulation time is 81 x 5 = {len(u.trajectory)*u.trajectory.dt} ps\\n   The paper reports saving a frame every 400ps which mean in this work we can only reproduce the first frame , equivilant to frame number 200.\\n   However to test the code, more frames are needed, therefore, a frame every 40ps will be selected. \")\n",
    "# protein = u.select_atoms(\"protein\")\n",
    "\n",
    "\n",
    "# # Preprocess the MDAnalysis data into JAX-compatible arrays\n",
    "# positions_array = jnp.array([\n",
    "#     [atom.position for atom in protein.atoms] for ts in u.trajectory\n",
    "# ])\n",
    "\n",
    "# flat_residue_atoms = jnp.concatenate([jnp.array([atom.index for atom in residue.atoms]) for residue in protein.residues])\n",
    "# residue_starts = jnp.array([0] + [len(residue.atoms) for residue in protein.residues[:-1]]).cumsum()\n",
    "# residue_lengths = jnp.array([len(residue.atoms) for residue in protein.residues])\n",
    "\n",
    "\n",
    "# protein_residues = protein.residues\n",
    "# print(f\"The system has {len(protein_residues)} residues\")\n",
    "# residue_atoms = []\n",
    "# for residue in protein.residues:\n",
    "#     residue_atoms.append(residue.atoms)\n",
    "# n_residues = len(protein_residues)\n",
    "# for residue in protein_residues:\n",
    "#     print(f\"{ residue.resid}, {residue.resname}\")\n",
    "\n",
    "\n",
    "# Introducing the system\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Introducing the system\n",
    "u = mda.Universe(tpr_file, xtc_file)\n",
    "protein = u.select_atoms(\"protein\")\n",
    "\n",
    "# Preprocess residue atoms to a padded array\n",
    "residue_atoms_list = [\n",
    "    [atom.index for atom in residue.atoms]\n",
    "    for residue in protein.residues\n",
    "]\n",
    "\n",
    "# Pad the residues to the maximum number of atoms\n",
    "max_atoms_per_residue = max(len(atoms) for atoms in residue_atoms_list)\n",
    "residue_atoms_array = jnp.array([\n",
    "    jnp.pad(jnp.array(atoms), (0, max_atoms_per_residue - len(atoms)))\n",
    "    for atoms in residue_atoms_list\n",
    "])\n",
    "\n",
    "# ✅ Use JAX-compatible filtering\n",
    "def process_residue(atoms):\n",
    "    # Filter out padding (0 values) using jax.lax.select\n",
    "    non_zero_atoms = jax.lax.select(atoms > 0, atoms, jnp.zeros_like(atoms))\n",
    "    return non_zero_atoms\n",
    "\n",
    "# ✅ Apply `vmap` to process each residue\n",
    "processed_residues = jax.vmap(process_residue)(residue_atoms_array)\n",
    "\n",
    "# Print the processed residues\n",
    "for i, residue in enumerate(protein.residues):\n",
    "    print(f\"Residue {residue.resid}, {residue.resname}: Processed Atoms - {processed_residues[i][processed_residues[i] > 0]}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The distance matrix, a direct function of the positions\n",
    "def compute_distance_matrix(positions):\n",
    "    \"\"\"\n",
    "    Compute the pairwise distance matrix for a set of atomic positions.\n",
    "\n",
    "    The distance matrix is a symmetric matrix where the element (i, j) represents \n",
    "    the Euclidean distance between atom `i` and atom `j`. This function is a direct \n",
    "    function of the positions provided.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    positions : jnp.ndarray\n",
    "        A 2D array of shape (n_atoms, 3) containing the Cartesian coordinates \n",
    "        (x, y, z) of each atom.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    distance_matrix : jnp.ndarray\n",
    "        A symmetric matrix of shape (n_atoms, n_atoms) where each element \n",
    "        represents the pairwise Euclidean distance between atoms.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    - The diagonal elements of the distance matrix are all zeros, as the distance \n",
    "      between an atom and itself is zero.\n",
    "    - The function uses JAX for efficient computation, enabling potential \n",
    "      acceleration via just-in-time compilation or parallelization.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    n_atoms = positions.shape[0]\n",
    "    distance_matrix = jnp.zeros((n_atoms, n_atoms))\n",
    "\n",
    "    for i in range(n_atoms):\n",
    "        for j in range(i + 1, n_atoms):\n",
    "            r = jnp.linalg.norm(positions[i] - positions[j])\n",
    "            distance_matrix = distance_matrix.at[i, j].set(r)\n",
    "            distance_matrix = distance_matrix.at[j, i].set(r)\n",
    "    \n",
    "    return distance_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_interaction_energy_matrix(distance_matrix, flat_residue_atoms, residue_starts, residue_lengths, lj_params):\n",
    "    \"\"\"\n",
    "    Compute the pairwise interaction energy matrix for non-contact interactions based on a precomputed distance matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    distance_matrix : jnp.ndarray\n",
    "        A precomputed symmetric matrix of pairwise Euclidean distances between atoms.\n",
    "    flat_residue_atoms : jnp.ndarray\n",
    "        Flattened array of all atomic indices for residues.\n",
    "    residue_starts : jnp.ndarray\n",
    "        Array of starting indices for each residue in `flat_residue_atoms`.\n",
    "    residue_lengths : jnp.ndarray\n",
    "        Array of the number of atoms in each residue.\n",
    "    lj_params : dict\n",
    "        A dictionary containing Lennard-Jones parameters for atom types and their charges.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    energy_matrix : jnp.ndarray\n",
    "        A symmetric matrix of shape (n_residues, n_residues), where each element represents\n",
    "        the total interaction energy (Lennard-Jones + Coulomb) between two residues.\n",
    "    \"\"\"\n",
    "    n_residues = len(residue_starts)\n",
    "    energy_matrix = jnp.zeros((n_residues, n_residues))\n",
    "    conversion_factor = 138.935485  # Electric conversion factor in KJ.mol.e^-2.nm^-1\n",
    "\n",
    "    def residue_pair_energy(i, j):\n",
    "        start_i = jnp.take(residue_starts, i, axis=0)\n",
    "        length_i = jnp.take(residue_lengths, i, axis=0)\n",
    "        start_j = jnp.take(residue_starts, j, axis=0)\n",
    "        length_j = jnp.take(residue_lengths, j, axis=0)\n",
    "\n",
    "\n",
    "        # Use lax.dynamic_slice_in_dim for dynamic slicing\n",
    "        residue_i = lax.dynamic_slice(flat_residue_atoms, (start_i,), (length_i,))\n",
    "        residue_j = lax.dynamic_slice(flat_residue_atoms, (start_j,), (length_j,))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        E_vdw = 0.0\n",
    "        E_coul = 0.0\n",
    "\n",
    "        for atom_i in residue_i:\n",
    "            for atom_j in residue_j:\n",
    "                r = distance_matrix[atom_i, atom_j]\n",
    "                nonzero_r = jnp.where(r > 0, r, 1e-8)\n",
    "\n",
    "                type_i = lj_params.get(atom_i, {}).get(\"type\")\n",
    "                type_j = lj_params.get(atom_j, {}).get(\"type\")\n",
    "                if type_i and type_j:\n",
    "                    c6_i, c12_i = lj_params[type_i][\"c6\"], lj_params[type_i][\"c12\"]\n",
    "                    c6_j, c12_j = lj_params[type_j][\"c6\"], lj_params[type_j][\"c12\"]\n",
    "\n",
    "                    sigma_i = (c12_i / c6_i) ** (1 / 6)\n",
    "                    epsilon_i = c6_i**2 / (4 * c12_i)\n",
    "                    sigma_j = (c12_j / c6_j) ** (1 / 6)\n",
    "                    epsilon_j = c6_j**2 / (4 * c12_j)\n",
    "\n",
    "                    sigma_ij = (sigma_i + sigma_j) / 2\n",
    "                    epsilon_ij = jnp.sqrt(epsilon_i * epsilon_j)\n",
    "\n",
    "                    E_vdw += jnp.where(\n",
    "                        r > 0,\n",
    "                        4 * epsilon_ij * ((sigma_ij / nonzero_r) ** 12 - (sigma_ij / nonzero_r) ** 6),\n",
    "                        0.0\n",
    "                    )\n",
    "\n",
    "                q_i = lj_params.get(atom_i, {}).get(\"charge\", 0.0)\n",
    "                q_j = lj_params.get(atom_j, {}).get(\"charge\", 0.0)\n",
    "                E_coul += jnp.where(\n",
    "                    r > 0,\n",
    "                    conversion_factor * (q_i * q_j) / nonzero_r,\n",
    "                    0.0\n",
    "                )\n",
    "\n",
    "        return E_vdw + E_coul\n",
    "\n",
    "    def residue_interaction(carry, pair):\n",
    "        i, j = pair\n",
    "        total_energy = residue_pair_energy(i, j)\n",
    "        return carry.at[i, j].set(total_energy).at[j, i].set(total_energy), None\n",
    "\n",
    "    energy_matrix, _ = lax.scan(residue_interaction, energy_matrix, jnp.triu_indices(n_residues, 1))\n",
    "\n",
    "    return energy_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Compute Eigenvalues, ENG, and SDENG\n",
    "def compute_eigenvalues(energy_matrix):\n",
    "    \"\"\"\n",
    "    Compute the eigenvalues of the interaction energy matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    energy_matrix : jnp.ndarray\n",
    "        A symmetric interaction energy matrix of shape (n_residues, n_residues).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    eigenvalues : jnp.ndarray\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    The eigenvalues are computed using JAX's `linalg.eigh` method.\n",
    "    \"\"\"\n",
    "    return jnp.linalg.eigh(energy_matrix)[0]\n",
    "\n",
    "\n",
    "\n",
    "def compute_eng_sdeng(eigenvalues):\n",
    "    \"\"\"\n",
    "    Compute the normalized energy gap (ENG) and the standard deviation of eigenvalues (SDENG).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    eigenvalues : jnp.ndarray\n",
    "        A 1D array of eigenvalues obtained from the interaction energy matrix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    eng : float\n",
    "        The normalized energy gap, calculated as the spectral gap divided by the\n",
    "        average separation between adjacent eigenvalues. Returns 0.0 if the average\n",
    "        separation is non-positive.\n",
    "    sdeng : float\n",
    "        The standard deviation of the eigenvalues.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The spectral gap is the difference between the two most negative eigenvalues.\n",
    "    - The average separation is computed as the mean of the differences between adjacent\n",
    "      eigenvalues.\n",
    "    \"\"\"\n",
    "    sorted_eigenvalues = jnp.sort(eigenvalues)\n",
    "    spectral_gap = sorted_eigenvalues[1] - sorted_eigenvalues[0]\n",
    "    avg_separation = jnp.mean(jnp.diff(sorted_eigenvalues))\n",
    "\n",
    "    eng = jnp.where(avg_separation > 0, spectral_gap / avg_separation, 0.0)\n",
    "    sdeng = jnp.std(eigenvalues)\n",
    "    return eng, sdeng\n",
    "\n",
    "# Compute Collective Variable (CV)\n",
    "def compute_cv(eng, sdeng, alpha, beta):\n",
    "    \"\"\"\n",
    "    Compute the Collective Variable (CV) as a linear combination of ENG and SDENG.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    eng : float\n",
    "        The spectral gap, computed from eigenvalues.\n",
    "    sdeng : float\n",
    "        The standard deviation of the eigenvalues.\n",
    "    alpha : float\n",
    "        Weighting factor for the ENG term.\n",
    "    beta : float\n",
    "        Weighting factor for the SDENG term.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The computed Collective Variable (CV) value, given by:\n",
    "        CV = alpha * ENG - beta * SDENG\n",
    "    \"\"\"\n",
    "    return alpha * eng - beta * sdeng\n",
    "\n",
    "# Modulate Weights\n",
    "# def modulate_weights_with_probability(eng_series, sdeng_series, frame_idx, alpha, beta, percentages):\n",
    "#     \"\"\"\n",
    "#     Modulate the weights (alpha and beta) based on ENG and SDENG values at a given frame,\n",
    "#     relative to dynamically computed thresholds.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     eng_series : jnp.ndarray\n",
    "#         Time series of ENG (spectral gap) values across frames.\n",
    "#     sdeng_series : jnp.ndarray\n",
    "#         Time series of SDENG (standard deviation of eigenvalues) values across frames.\n",
    "#     frame_idx : int\n",
    "#         The current frame index in the simulation.\n",
    "#     alpha : float\n",
    "#         The current weight for the ENG term.\n",
    "#     beta : float\n",
    "#         The current weight for the SDENG term.\n",
    "#     percentages : list of float\n",
    "#         A list of percentages used to adjust the thresholds for modulation.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     tuple of (float, float)\n",
    "#         Updated values of alpha and beta after applying the modulation rule.\n",
    "\n",
    "   \n",
    "#     \"\"\"\n",
    "#     # Modulate weights based on probabilities\n",
    "\n",
    "#     eng_max = jnp.max(eng_series)\n",
    "#     sdeng_min = jnp.min(sdeng_series)\n",
    "#     msd = jnp.std(eng_series)\n",
    "\n",
    "#     n = percentages[frame_idx % len(percentages)]\n",
    "#     eng_threshold = eng_max - (n / 100) * msd\n",
    "#     sdeng_threshold = sdeng_min + (n / 100) * msd\n",
    "\n",
    "#     eng_prob = eng_series[frame_idx] > eng_threshold\n",
    "#     sdeng_prob = sdeng_series[frame_idx] < sdeng_threshold\n",
    "\n",
    "#     if eng_prob:\n",
    "#         alpha *= 1.1\n",
    "#     if sdeng_prob:\n",
    "#         beta *= 0.9\n",
    "\n",
    "#     return alpha, beta\n",
    "\n",
    "\n",
    "\n",
    "def modulate_weights_with_probability(eng_series, sdeng_series, frame_idx, alpha, beta, percentages):\n",
    "    \"\"\"\n",
    "    Modulate the weights (alpha and beta) based on ENG and SDENG values at a given frame,\n",
    "    relative to dynamically computed thresholds.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    eng_series : jnp.ndarray\n",
    "        Time series of ENG (spectral gap) values across frames.\n",
    "    sdeng_series : jnp.ndarray\n",
    "        Time series of SDENG (standard deviation of eigenvalues) values across frames.\n",
    "    frame_idx : int\n",
    "        The current frame index in the simulation.\n",
    "    alpha : float\n",
    "        The current weight for the ENG term.\n",
    "    beta : float\n",
    "        The current weight for the SDENG term.\n",
    "    percentages : jnp.ndarray\n",
    "        A JAX array of percentages used to adjust the thresholds for modulation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of (float, float)\n",
    "        Updated values of alpha and beta after applying the modulation rule.\n",
    "    \"\"\"\n",
    "    # Compute thresholds\n",
    "    eng_max = jnp.max(eng_series)\n",
    "    sdeng_min = jnp.min(sdeng_series)\n",
    "    msd = jnp.std(eng_series)\n",
    "\n",
    "    n = percentages[frame_idx % len(percentages)]\n",
    "    eng_threshold = eng_max - (n / 100) * msd\n",
    "    sdeng_threshold = sdeng_min + (n / 100) * msd\n",
    "\n",
    "    # Compute probabilities\n",
    "    eng_prob = eng_series[frame_idx] > eng_threshold\n",
    "    sdeng_prob = sdeng_series[frame_idx] < sdeng_threshold\n",
    "\n",
    "    # Use JAX conditional logic for alpha and beta updates\n",
    "    alpha = jnp.where(eng_prob, alpha * 1.1, alpha)\n",
    "    beta = jnp.where(sdeng_prob, beta * 0.9, beta)\n",
    "\n",
    "    return alpha, beta\n",
    "\n",
    "\n",
    "\n",
    "# def compute_cv_and_dependencies(positions, n_atoms, lj_params, alpha, beta):\n",
    "#     \"\"\"\n",
    "#     Compute CV and its dependencies based on atomic positions.\n",
    "\n",
    "#     Parameters:\n",
    "#     ----------\n",
    "#     positions : jnp.ndarray\n",
    "#         Atomic positions (n_atoms, 3).\n",
    "#     n_atoms : int\n",
    "#         Number of atoms.\n",
    "#     lj_params : dict\n",
    "#         Lennard-Jones parameters.\n",
    "#     alpha : float\n",
    "#         Weight for ENG.\n",
    "#     beta : float\n",
    "#         Weight for SDENG.\n",
    "\n",
    "#     Returns:\n",
    "#     -------\n",
    "#     tuple\n",
    "#         CV value, ENG, SDENG, and other dependencies.\n",
    "#     \"\"\"\n",
    "#     distance_matrix = compute_distance_matrix(positions)\n",
    "\n",
    "#     energy_matrix = compute_interaction_energy_matrix(distance_matrix, residue_atoms, lj_params)\n",
    "\n",
    "#     eigenvalues = compute_eigenvalues(energy_matrix)\n",
    "#     eng, sdeng = compute_eng_sdeng(eigenvalues)\n",
    "\n",
    "#     cv_value = compute_cv(eng, sdeng, alpha, beta)\n",
    "\n",
    "#     return cv_value, eng, sdeng, distance_matrix, energy_matrix\n",
    "def compute_cv_and_dependencies(positions, n_atoms, lj_params, alpha, beta, flat_residue_atoms, residue_starts, residue_lengths):\n",
    "    \"\"\"\n",
    "    Compute CV and its dependencies based on atomic positions.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    positions : jnp.ndarray\n",
    "        Atomic positions (n_atoms, 3).\n",
    "    n_atoms : int\n",
    "        Number of atoms.\n",
    "    lj_params : dict\n",
    "        Lennard-Jones parameters.\n",
    "    alpha : float\n",
    "        Weight for ENG.\n",
    "    beta : float\n",
    "        Weight for SDENG.\n",
    "    flat_residue_atoms : jnp.ndarray\n",
    "        Flattened array of all atomic indices for residues.\n",
    "    residue_starts : jnp.ndarray\n",
    "        Array of starting indices for each residue in `flat_residue_atoms`.\n",
    "    residue_lengths : jnp.ndarray\n",
    "        Array of the number of atoms in each residue.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    tuple\n",
    "        CV value, ENG, SDENG, distance_matrix, and energy_matrix.\n",
    "    \"\"\"\n",
    "    # Compute distance matrix\n",
    "    distance_matrix = compute_distance_matrix(positions)\n",
    "\n",
    "    # Compute energy matrix with additional arguments for residue-level interactions\n",
    "    energy_matrix = compute_interaction_energy_matrix(\n",
    "        distance_matrix, flat_residue_atoms, residue_starts, residue_lengths, lj_params\n",
    "    )\n",
    "\n",
    "    # Compute eigenvalues, ENG, and SDENG\n",
    "    eigenvalues = compute_eigenvalues(energy_matrix)\n",
    "    eng, sdeng = compute_eng_sdeng(eigenvalues)\n",
    "\n",
    "    # Compute the Collective Variable (CV)\n",
    "    cv_value = compute_cv(eng, sdeng, alpha, beta)\n",
    "\n",
    "    return cv_value, eng, sdeng, distance_matrix, energy_matrix\n",
    "\n",
    "# def compute_cv_and_dependencies(positions, n_atoms, lj_params, alpha, beta, flat_residue_atoms, residue_starts, residue_lengths):\n",
    "#     \"\"\"\n",
    "#     Compute CV and its dependencies based on atomic positions.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     positions : jnp.ndarray\n",
    "#         Atomic positions (n_atoms, 3).\n",
    "#     n_atoms : int\n",
    "#         Number of atoms.\n",
    "#     lj_params : dict\n",
    "#         Lennard-Jones parameters.\n",
    "#     alpha : float\n",
    "#         Weight for ENG.\n",
    "#     beta : float\n",
    "#         Weight for SDENG.\n",
    "#     flat_residue_atoms : jnp.ndarray\n",
    "#         Flattened array of all atomic indices for residues.\n",
    "#     residue_starts : jnp.ndarray\n",
    "#         Array of starting indices for each residue in `flat_residue_atoms`.\n",
    "#     residue_lengths : jnp.ndarray\n",
    "#         Array of the number of atoms in each residue.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     tuple\n",
    "#         CV value, ENG, SDENG, and other dependencies.\n",
    "#     \"\"\"\n",
    "#     distance_matrix = compute_distance_matrix(positions)\n",
    "\n",
    "#     energy_matrix = compute_interaction_energy_matrix(\n",
    "#         distance_matrix, flat_residue_atoms, residue_starts, residue_lengths, lj_params\n",
    "#     )\n",
    "\n",
    "#     eigenvalues = compute_eigenvalues(energy_matrix)\n",
    "#     eng, sdeng = compute_eng_sdeng(eigenvalues)\n",
    "\n",
    "#     cv_value = compute_cv(eng, sdeng, alpha, beta)\n",
    "\n",
    "#     return cv_value, eng, sdeng, distance_matrix, energy_matrix\n",
    "\n",
    "\n",
    "\n",
    "# @jax.jit\n",
    "# def compute_cv_gradient_per_frame(positions, n_atoms, lj_params, alpha, beta):\n",
    "#     \"\"\"\n",
    "#     Compute the gradient of the CV with respect to atomic positions for one frame.\n",
    "\n",
    "#     Parameters:\n",
    "#     ----------\n",
    "#     positions : jnp.ndarray\n",
    "#         Atomic positions (n_atoms, 3).\n",
    "#     n_atoms : int\n",
    "#         Number of atoms.\n",
    "#     lj_params : dict\n",
    "#         Lennard-Jones parameters.\n",
    "#     alpha : float\n",
    "#         Weight for ENG.\n",
    "#     beta : float\n",
    "#         Weight for SDENG.\n",
    "\n",
    "#     Returns:\n",
    "#     -------\n",
    "#     jnp.ndarray\n",
    "#         Gradient of the CV with respect to atomic positions.\n",
    "#     \"\"\"\n",
    "#     def cv_function(positions_flat):\n",
    "#         # Reshape positions to (n_atoms, 3)\n",
    "#         positions = positions_flat.reshape((n_atoms, 3))\n",
    "#         cv_value, _, _, _, _ = compute_cv_and_dependencies(positions, n_atoms, lj_params, alpha, beta)\n",
    "#         return cv_value\n",
    "\n",
    "#     # Flatten positions for JAX compatibility\n",
    "#     positions_flat = positions.flatten()\n",
    "#     return grad(cv_function)(positions_flat).reshape((n_atoms, 3))\n",
    "# @jax,jit\n",
    "# def compute_cv_gradient_per_frame(positions, n_atoms, lj_params, alpha, beta):\n",
    "#     \"\"\"\n",
    "#     Compute the gradient of the CV with respect to atomic positions for one frame.\n",
    "#     \"\"\"\n",
    "#     def cv_function(positions_flat):\n",
    "#         # Reshape positions to (n_atoms, 3)\n",
    "#         positions = positions_flat.reshape((n_atoms, 3))\n",
    "#         cv_value, _, _, _, _ = compute_cv_and_dependencies(positions, n_atoms, lj_params, alpha, beta)\n",
    "#         return cv_value\n",
    "\n",
    "#     # Flatten positions for JAX compatibility\n",
    "#     positions_flat = positions.flatten()\n",
    "#     return jax.jit(grad(cv_function), static_argnums=(2,))(positions_flat).reshape((n_atoms, 3))\n",
    "\n",
    "\n",
    "def compute_cv_gradient_per_frame(positions, n_atoms, lj_params, alpha, beta):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the CV with respect to atomic positions for one frame.\n",
    "    \"\"\"\n",
    "    def cv_function(positions_flat):\n",
    "        positions = positions_flat.reshape((n_atoms, 3))\n",
    "        cv_value, _, _, _, _ = compute_cv_and_dependencies(positions, n_atoms, lj_params, alpha, beta)\n",
    "        return cv_value\n",
    "\n",
    "    positions_flat = positions.flatten()\n",
    "    gradient_function = grad(cv_function)\n",
    "    return jax.jit(gradient_function)(positions_flat).reshape((n_atoms, 3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "\n",
    "@jax.jit\n",
    "def main(positions_array, lj_params_tuple, percentages, flat_residue_atoms, residue_starts, residue_lengths):\n",
    "    \"\"\"\n",
    "    JIT-compiled main function to compute the Collective Variable (CV), its gradient,\n",
    "    and modulate weights for a molecular dynamics (MD) trajectory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    positions_array : jnp.ndarray\n",
    "        A 3D array of shape (n_frames, n_atoms, 3) containing atomic positions for each frame.\n",
    "    lj_params : dict\n",
    "        Lennard-Jones interaction parameters for the simulation.\n",
    "    percentages : jnp.ndarray\n",
    "        A 1D array of percentages used to adjust modulation thresholds.\n",
    "    flat_residue_atoms : jnp.ndarray\n",
    "        Flattened array of all atomic indices for residues.\n",
    "    residue_starts : jnp.ndarray\n",
    "        Array of starting indices for each residue in `flat_residue_atoms`.\n",
    "    residue_lengths : jnp.ndarray\n",
    "        Array of the number of atoms in each residue.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    lj_params = dict(lj_params_tuple)\n",
    "    n_frames = positions_array.shape[0]\n",
    "    n_atoms = positions_array.shape[1]\n",
    "\n",
    "    alpha, beta = 1.0, 1.0\n",
    "    eng_time_series = jnp.zeros(n_frames)\n",
    "    sdeng_time_series = jnp.zeros(n_frames)\n",
    "\n",
    "    def frame_step(carry, frame_idx):\n",
    "        alpha, beta, eng_time_series, sdeng_time_series = carry\n",
    "        positions = positions_array[frame_idx]\n",
    "\n",
    "        # Compute CV and Dependencies\n",
    "        cv_value, eng, sdeng, _, _ = compute_cv_and_dependencies(\n",
    "            positions, n_atoms, lj_params, alpha, beta, flat_residue_atoms, residue_starts, residue_lengths\n",
    "        )\n",
    "\n",
    "        # Store Time Series Data\n",
    "        eng_time_series = eng_time_series.at[frame_idx].set(eng)\n",
    "        sdeng_time_series = sdeng_time_series.at[frame_idx].set(sdeng)\n",
    "\n",
    "        # Modulate Weights\n",
    "        alpha, beta = modulate_weights_with_probability(\n",
    "            eng_time_series, sdeng_time_series, frame_idx, alpha, beta, percentages\n",
    "        )\n",
    "\n",
    "        # Compute CV Gradient\n",
    "        cv_grad = compute_cv_gradient_per_frame(positions, n_atoms, lj_params, alpha, beta)\n",
    "\n",
    "        # Print results\n",
    "        print(f\"Frame {frame_idx}: CV(T)={cv_value}, ENG(T)={eng}, SDENG(T)={sdeng}, Alpha={alpha}, Beta={beta}\")\n",
    "        print(f\"Frame {frame_idx}: Gradient Norm = {jnp.linalg.norm(cv_grad)}\")\n",
    "\n",
    "        return (alpha, beta, eng_time_series, sdeng_time_series), None\n",
    "\n",
    "    # Initialize carry\n",
    "    carry_init = (alpha, beta, eng_time_series, sdeng_time_series)\n",
    "\n",
    "    # Use lax.scan for efficient looping\n",
    "    carry_final, _ = lax.scan(frame_step, carry_init, jnp.arange(n_frames))\n",
    "\n",
    "    return carry_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Shapes must be 1D sequences of concrete values of integer type, got (Traced<ShapedArray(int32[])>with<DynamicJaxprTrace>,).\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThe error occurred while tracing the function residue_interaction at /var/folders/t5/cty4pmdx4fzgbbz0ptr_q2z40000gn/T/ipykernel_2574/3660929749.py:80 for scan. This concrete value was not available in Python because it depends on the value of the argument pair[0].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[225], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Call the JIT-compiled function\u001b[39;00m\n\u001b[1;32m      5\u001b[0m main_jit \u001b[38;5;241m=\u001b[39m partial(jax\u001b[38;5;241m.\u001b[39mjit(main, static_argnames\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlj_params_tuple\u001b[39m\u001b[38;5;124m\"\u001b[39m,)))\n\u001b[0;32m----> 6\u001b[0m \u001b[43mmain_jit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositions_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlj_params_tuple\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpercentages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_residue_atoms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidue_starts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidue_lengths\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 28 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[224], line 69\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(positions_array, lj_params_tuple, percentages, flat_residue_atoms, residue_starts, residue_lengths)\u001b[0m\n\u001b[1;32m     66\u001b[0m carry_init \u001b[38;5;241m=\u001b[39m (alpha, beta, eng_time_series, sdeng_time_series)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Use lax.scan for efficient looping\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m carry_final, _ \u001b[38;5;241m=\u001b[39m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcarry_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_frames\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m carry_final\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[224], line 43\u001b[0m, in \u001b[0;36mmain.<locals>.frame_step\u001b[0;34m(carry, frame_idx)\u001b[0m\n\u001b[1;32m     40\u001b[0m positions \u001b[38;5;241m=\u001b[39m positions_array[frame_idx]\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Compute CV and Dependencies\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m cv_value, eng, sdeng, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_cv_and_dependencies\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_atoms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlj_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_residue_atoms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidue_starts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidue_lengths\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Store Time Series Data\u001b[39;00m\n\u001b[1;32m     48\u001b[0m eng_time_series \u001b[38;5;241m=\u001b[39m eng_time_series\u001b[38;5;241m.\u001b[39mat[frame_idx]\u001b[38;5;241m.\u001b[39mset(eng)\n",
      "Cell \u001b[0;32mIn[223], line 332\u001b[0m, in \u001b[0;36mcompute_cv_and_dependencies\u001b[0;34m(positions, n_atoms, lj_params, alpha, beta, flat_residue_atoms, residue_starts, residue_lengths)\u001b[0m\n\u001b[1;32m    329\u001b[0m distance_matrix \u001b[38;5;241m=\u001b[39m compute_distance_matrix(positions)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# Compute energy matrix with additional arguments for residue-level interactions\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m energy_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_interaction_energy_matrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistance_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_residue_atoms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidue_starts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidue_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlj_params\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# Compute eigenvalues, ENG, and SDENG\u001b[39;00m\n\u001b[1;32m    337\u001b[0m eigenvalues \u001b[38;5;241m=\u001b[39m compute_eigenvalues(energy_matrix)\n",
      "Cell \u001b[0;32mIn[223], line 85\u001b[0m, in \u001b[0;36mcompute_interaction_energy_matrix\u001b[0;34m(distance_matrix, flat_residue_atoms, residue_starts, residue_lengths, lj_params)\u001b[0m\n\u001b[1;32m     82\u001b[0m     total_energy \u001b[38;5;241m=\u001b[39m residue_pair_energy(i, j)\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m carry\u001b[38;5;241m.\u001b[39mat[i, j]\u001b[38;5;241m.\u001b[39mset(total_energy)\u001b[38;5;241m.\u001b[39mat[j, i]\u001b[38;5;241m.\u001b[39mset(total_energy), \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m energy_matrix, _ \u001b[38;5;241m=\u001b[39m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresidue_interaction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menergy_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtriu_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_residues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m energy_matrix\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[223], line 82\u001b[0m, in \u001b[0;36mcompute_interaction_energy_matrix.<locals>.residue_interaction\u001b[0;34m(carry, pair)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresidue_interaction\u001b[39m(carry, pair):\n\u001b[1;32m     81\u001b[0m     i, j \u001b[38;5;241m=\u001b[39m pair\n\u001b[0;32m---> 82\u001b[0m     total_energy \u001b[38;5;241m=\u001b[39m \u001b[43mresidue_pair_energy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m carry\u001b[38;5;241m.\u001b[39mat[i, j]\u001b[38;5;241m.\u001b[39mset(total_energy)\u001b[38;5;241m.\u001b[39mat[j, i]\u001b[38;5;241m.\u001b[39mset(total_energy), \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[223], line 36\u001b[0m, in \u001b[0;36mcompute_interaction_energy_matrix.<locals>.residue_pair_energy\u001b[0;34m(i, j)\u001b[0m\n\u001b[1;32m     32\u001b[0m length_j \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mtake(residue_lengths, j, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Use lax.dynamic_slice_in_dim for dynamic slicing\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m residue_i \u001b[38;5;241m=\u001b[39m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdynamic_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_residue_atoms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m residue_j \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39mdynamic_slice(flat_residue_atoms, (start_j,), (length_j,))\n\u001b[1;32m     42\u001b[0m E_vdw \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/jax/_src/core.py:1567\u001b[0m, in \u001b[0;36mcanonicalize_shape\u001b[0;34m(shape, context)\u001b[0m\n\u001b[1;32m   1565\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1566\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m-> 1567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _invalid_shape_error(shape, context)\n",
      "\u001b[0;31mTypeError\u001b[0m: Shapes must be 1D sequences of concrete values of integer type, got (Traced<ShapedArray(int32[])>with<DynamicJaxprTrace>,).\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThe error occurred while tracing the function residue_interaction at /var/folders/t5/cty4pmdx4fzgbbz0ptr_q2z40000gn/T/ipykernel_2574/3660929749.py:80 for scan. This concrete value was not available in Python because it depends on the value of the argument pair[0]."
     ]
    }
   ],
   "source": [
    "# Convert percentages list to a JAX array\n",
    "percentages = jnp.array(list(range(0, 101, 1)))\n",
    "lj_params_tuple = tuple(lj_params.items())\n",
    "# Call the JIT-compiled function\n",
    "main_jit = partial(jax.jit(main, static_argnames=(\"lj_params_tuple\",)))\n",
    "main_jit(positions_array, lj_params_tuple, percentages, flat_residue_atoms, residue_starts, residue_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim_run = main_jit(u, protein,lj_params)\n",
    "# sim_run\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File written successfully!\n"
     ]
    }
   ],
   "source": [
    "# # Define the content you want to write\n",
    "# content = \"\"\"\n",
    "\n",
    "# LOAD FILE=/path/to/plumed2/plugins/pycv/PythonCVInterface.so\n",
    "# rc: PYCVINTERFACE ATOMS=1,2,3 IMPORT=curvature CALCULATE=plumedCalculate\n",
    "# lr: RESTRAINT ARG=d KAPPA=0 AT=0 SLOPE=2.5\n",
    "# PRINT ARG=rc FILE=rc_colvar.out STRIDE=1\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "# # Write the content to a file\n",
    "# with open(\"rc_input.dat\", \"w\") as file:\n",
    "#     file.write(content)\n",
    "\n",
    "# print(\"File written successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX Gradient:\n",
      " [[ 2.  4.  6.]\n",
      " [ 8. 10. 12.]]\n",
      "Finite Difference Gradient:\n",
      " [[ 1.9989012  3.9978025  5.9967036]\n",
      " [ 8.003235  10.002136  12.001037 ]]\n",
      "Difference:\n",
      " [[ 0.00109875  0.0021975   0.00329638]\n",
      " [-0.00323486 -0.00213623 -0.00103664]]\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "import numpy as np\n",
    "\n",
    "# Function to compute finite difference approximation\n",
    "def finite_difference(func, positions, h=1e-3):\n",
    "    n_atoms, _ = positions.shape\n",
    "    grad_approx = np.zeros_like(positions)\n",
    "    \n",
    "    for i in range(n_atoms):\n",
    "        for j in range(3):  # x, y, z components\n",
    "            positions_plus = positions.copy()\n",
    "            positions_plus[i, j] += h\n",
    "            grad_approx[i, j] = (func(positions_plus) - func(positions)) / h\n",
    "            \n",
    "    return grad_approx\n",
    "\n",
    "# Example CV function for testing\n",
    "def compute_cv(positions_flat):\n",
    "    positions = positions_flat.reshape((-1, 3))\n",
    "    return jnp.sum(positions**2)  # Simple test function: sum of squares\n",
    "\n",
    "# Compute JAX gradient\n",
    "positions = jnp.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], dtype = jnp.float32)\n",
    "jax_grad = grad(compute_cv)(positions.flatten()).reshape((-1, 3))\n",
    "\n",
    "# Compute finite difference gradient\n",
    "fd_grad = finite_difference(lambda x: np.sum(x**2), np.array(positions))\n",
    "\n",
    "# Compare results\n",
    "print(\"JAX Gradient:\\n\", jax_grad)\n",
    "print(\"Finite Difference Gradient:\\n\", fd_grad)\n",
    "print(\"Difference:\\n\", jax_grad - fd_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX Gradient:\n",
      " [[ 2.  4.  6.]\n",
      " [ 8. 10. 12.]]\n",
      "Finite Difference Gradient:\n",
      " [[ 1.9836426  3.9672852  5.9509277]\n",
      " [ 8.010864   9.994507  12.054443 ]]\n",
      "Symbolic Gradient:\n",
      " [[2.00000000000000 4.00000000000000 6.00000000000000]\n",
      " [8.00000000000000 10.0000000000000 12.0000000000000]]\n",
      "\n",
      "Difference (JAX - Finite Difference):\n",
      " [[ 0.01635742  0.03271484  0.04907227]\n",
      " [-0.01086426  0.00549316 -0.05444336]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Error interpreting argument to <function subtract at 0x11bea1090> as an abstract array. The problematic value is of type <class 'numpy.ndarray'> and was passed to the function at path y.\nThis typically means that a jit-wrapped function was called with a non-array argument, and this argument was not marked as static using the static_argnums or static_argnames parameters of jax.jit.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/jax/_src/api_util.py:607\u001b[0m, in \u001b[0;36mshaped_abstractify\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    606\u001b[0m handler \u001b[38;5;241m=\u001b[39m _shaped_abstractify_handlers\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mtype\u001b[39m(x), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 607\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m _shaped_abstractify_slow(x)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/jax/_src/api_util.py:618\u001b[0m, in \u001b[0;36m_numpy_array_abstractify\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    617\u001b[0m dtype \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m--> 618\u001b[0m \u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_valid_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ShapedArray(x\u001b[38;5;241m.\u001b[39mshape,\n\u001b[1;32m    620\u001b[0m     dtypes\u001b[38;5;241m.\u001b[39mcanonicalize_dtype(dtype, allow_extended_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/jax/_src/dtypes.py:739\u001b[0m, in \u001b[0;36mcheck_valid_dtype\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _jax_dtype_set:\n\u001b[0;32m--> 739\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid JAX array \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    740\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype. Only arrays of numeric types are supported by JAX.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Dtype object is not a valid JAX array type. Only arrays of numeric types are supported by JAX.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 64\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Print differences\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDifference (JAX - Finite Difference):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, jax_grad \u001b[38;5;241m-\u001b[39m fd_grad)\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDifference (JAX - Symbolic):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[43mjax_grad\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msym_grad\u001b[49m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:573\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    571\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 573\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/jax/_src/numpy/ufunc_api.py:179\u001b[0m, in \u001b[0;36mufunc.__call__\u001b[0;34m(self, out, where, *args)\u001b[0m\n\u001b[1;32m    177\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere argument of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__static_props[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_vectorized\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 4 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/jax/_src/pjit.py:626\u001b[0m, in \u001b[0;36m_infer_params_impl\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    624\u001b[0m       arg_description \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdbg\u001b[38;5;241m.\u001b[39marg_names[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dbg\n\u001b[1;32m    625\u001b[0m                          \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflattened argument number \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 626\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    627\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError interpreting argument to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfun\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as an abstract array.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    628\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m The problematic value is of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(a)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and was passed to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    629\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m the function at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_description\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis typically means that a jit-wrapped function was called with a non-array\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m argument, and this argument was not marked as static using the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m static_argnums or static_argnames parameters of jax.jit.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    633\u001b[0m       ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    635\u001b[0m   in_type \u001b[38;5;241m=\u001b[39m in_avals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(avals)\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: Error interpreting argument to <function subtract at 0x11bea1090> as an abstract array. The problematic value is of type <class 'numpy.ndarray'> and was passed to the function at path y.\nThis typically means that a jit-wrapped function was called with a non-array argument, and this argument was not marked as static using the static_argnums or static_argnames parameters of jax.jit."
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "import sympy as sp\n",
    "import numpy as np\n",
    "\n",
    "# Function to compute finite difference approximation\n",
    "def finite_difference(func, positions, h=1e-4):\n",
    "    n_atoms, _ = positions.shape\n",
    "    grad_approx = np.zeros_like(positions)\n",
    "    \n",
    "    for i in range(n_atoms):\n",
    "        for j in range(3):  # x, y, z components\n",
    "            positions_plus = positions.copy()\n",
    "            positions_plus[i, j] += h\n",
    "            grad_approx[i, j] = (func(positions_plus) - func(positions)) / h\n",
    "            \n",
    "    return grad_approx\n",
    "\n",
    "# Example CV function for testing\n",
    "def compute_cv(positions_flat):\n",
    "    positions = positions_flat.reshape((-1, 3))\n",
    "    return jnp.sum(positions**2)  # Simple test function: sum of squares\n",
    "\n",
    "# Symbolic differentiation using SymPy\n",
    "def symbolic_gradient(positions):\n",
    "    x, y, z, x2, y2, z2 = sp.symbols('x y z x2 y2 z2')\n",
    "    vars_flat = [x, y, z, x2, y2, z2]\n",
    "\n",
    "    # Define the symbolic function (sum of squares in this case)\n",
    "    func = sum(var**2 for var in vars_flat)\n",
    "\n",
    "    # Compute gradients symbolically\n",
    "    gradients = [sp.diff(func, var) for var in vars_flat]\n",
    "    \n",
    "    # Substitute values from positions\n",
    "    subs = {x: positions[0, 0], y: positions[0, 1], z: positions[0, 2],\n",
    "            x2: positions[1, 0], y2: positions[1, 1], z2: positions[1, 2]}\n",
    "    \n",
    "    # Evaluate the gradients at the given positions\n",
    "    grad_values = [grad.evalf(subs=subs) for grad in gradients]\n",
    "    \n",
    "    # Reshape into the same format as the positions array\n",
    "    return np.array(grad_values).reshape((-1, 3))\n",
    "\n",
    "# Initialize positions\n",
    "positions = jnp.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], dtype=jnp.float32)\n",
    "\n",
    "# Compute JAX gradient\n",
    "jax_grad = grad(compute_cv)(positions.flatten()).reshape((-1, 3))\n",
    "\n",
    "# Compute finite difference gradient\n",
    "fd_grad = finite_difference(lambda x: np.sum(x**2), np.array(positions))\n",
    "\n",
    "# Compute symbolic gradient\n",
    "sym_grad = symbolic_gradient(np.array(positions))\n",
    "\n",
    "# Compare results\n",
    "print(\"JAX Gradient:\\n\", jax_grad)\n",
    "print(\"Finite Difference Gradient:\\n\", fd_grad)\n",
    "print(\"Symbolic Gradient:\\n\", sym_grad)\n",
    "\n",
    "# Print differences\n",
    "print(\"\\nDifference (JAX - Finite Difference):\\n\", jax_grad - fd_grad)\n",
    "print(\"\\nDifference (JAX - Symbolic):\\n\", jax_grad - sym_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX Gradient:\n",
      " [[ 2.  4.  6.]\n",
      " [ 8. 10. 12.]]\n",
      "Finite Difference Gradient:\n",
      " [[ 1.9836426  3.9672852  5.9509277]\n",
      " [ 8.010864   9.994507  12.054443 ]]\n",
      "Symbolic Gradient:\n",
      " [[ 2.  4.  6.]\n",
      " [ 8. 10. 12.]]\n",
      "\n",
      "Difference (JAX - Finite Difference):\n",
      " [[ 0.01635742  0.03271484  0.04907227]\n",
      " [-0.01086426  0.00549316 -0.05444336]]\n",
      "\n",
      "Difference (JAX - Symbolic):\n",
      " [[0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "import sympy as sp\n",
    "import numpy as np\n",
    "\n",
    "# Function to compute finite difference approximation\n",
    "def finite_difference(func, positions, h=1e-4):\n",
    "    n_atoms, _ = positions.shape\n",
    "    grad_approx = np.zeros_like(positions)\n",
    "    \n",
    "    for i in range(n_atoms):\n",
    "        for j in range(3):  # x, y, z components\n",
    "            positions_plus = positions.copy()\n",
    "            positions_plus[i, j] += h\n",
    "            grad_approx[i, j] = (func(positions_plus) - func(positions)) / h\n",
    "            \n",
    "    return grad_approx\n",
    "\n",
    "# Example CV function for testing\n",
    "def compute_cv(positions_flat):\n",
    "    positions = positions_flat.reshape((-1, 3))\n",
    "    return jnp.sum(positions**2)  # Simple test function: sum of squares\n",
    "\n",
    "# Symbolic differentiation using SymPy\n",
    "def symbolic_gradient(positions):\n",
    "    x, y, z, x2, y2, z2 = sp.symbols('x y z x2 y2 z2')\n",
    "    vars_flat = [x, y, z, x2, y2, z2]\n",
    "\n",
    "    # Define the symbolic function (sum of squares in this case)\n",
    "    func = sum(var**2 for var in vars_flat)\n",
    "\n",
    "    # Compute gradients symbolically\n",
    "    gradients = [sp.diff(func, var) for var in vars_flat]\n",
    "    \n",
    "    # Substitute values from positions\n",
    "    subs = {x: positions[0, 0], y: positions[0, 1], z: positions[0, 2],\n",
    "            x2: positions[1, 0], y2: positions[1, 1], z2: positions[1, 2]}\n",
    "    \n",
    "    # Evaluate the gradients at the given positions\n",
    "    grad_values = [float(grad.evalf(subs=subs)) for grad in gradients]\n",
    "    \n",
    "    # Reshape into the same format as the positions array\n",
    "    return np.array(grad_values).reshape((-1, 3))\n",
    "\n",
    "# Initialize positions\n",
    "positions = jnp.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], dtype=jnp.float32)\n",
    "\n",
    "# Compute JAX gradient\n",
    "jax_grad = grad(compute_cv)(positions.flatten()).reshape((-1, 3))\n",
    "\n",
    "# Compute finite difference gradient\n",
    "fd_grad = finite_difference(lambda x: np.sum(x**2), np.array(positions))\n",
    "\n",
    "# Compute symbolic gradient, convert to NumPy, then to JAX array\n",
    "sym_grad = jnp.array(symbolic_gradient(np.array(positions)))\n",
    "\n",
    "# Compare results\n",
    "print(\"JAX Gradient:\\n\", jax_grad)\n",
    "print(\"Finite Difference Gradient:\\n\", fd_grad)\n",
    "print(\"Symbolic Gradient:\\n\", sym_grad)\n",
    "\n",
    "# Print differences\n",
    "print(\"\\nDifference (JAX - Finite Difference):\\n\", jax_grad - fd_grad)\n",
    "print(\"\\nDifference (JAX - Symbolic):\\n\", jax_grad - sym_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "my_list = [1,2,3,4,5]\n",
    "def square (x):\n",
    "    return x**2\n",
    "\n",
    "new_list = list(map(square, my_list))\n",
    "\n",
    "print(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 8, 27, 64, 125]\n"
     ]
    }
   ],
   "source": [
    "new_list = list(map(lambda x: x**3, my_list))\n",
    "print(new_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule 1: Jax chooses the last input if there is a tie, since only one input is needed while FD method fails to keep the functions integrity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX Gradient_x:\n",
      " [1. 0. 0.]\n",
      "Finite Difference Gradient_x:\n",
      " [0.99897385 0.         0.        ]\n",
      "JAX Gradient_y:\n",
      " [0. 0. 1.]\n",
      "Finite Difference Gradient_y:\n",
      " [0.99897385 0.99897385 0.99897385]\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "import numpy as np\n",
    "\n",
    "# Test function using sort\n",
    "def sort_function(x):\n",
    "    return jnp.sort(x)[-1]  # Returns the largest value after sorting\n",
    "\n",
    "# Finite difference gradient function\n",
    "def finite_difference_sort(func, x, h=1e-4):\n",
    "    grad_approx = np.zeros_like(x)\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        x_plus = x.copy()\n",
    "        x_plus[i] += h\n",
    "        grad_approx[i] = (func(x_plus) - func(x)) / h\n",
    "    \n",
    "    return grad_approx\n",
    "\n",
    "# Input array\n",
    "x = jnp.array([3.0, 1.0, 2.0])\n",
    "y = jnp.array([3.0, 3.0, 3.0])\n",
    "# Compute JAX gradient\n",
    "jax_grad_x = grad(sort_function)(x)\n",
    "jax_grad_y = grad(sort_function)(y)\n",
    "# Compute finite difference gradient\n",
    "fd_grad_x = finite_difference_sort(lambda x: np.sort(x)[-1], np.array(x))\n",
    "fd_grad_y = finite_difference_sort(lambda y: np.sort(y)[-1], np.array(y))\n",
    "# Print results\n",
    "print(\"JAX Gradient_x:\\n\", jax_grad_x)\n",
    "print(\"Finite Difference Gradient_x:\\n\", fd_grad_x)\n",
    "# print(\"Difference (JAX - Finite Difference):\\n\", jax_grad - fd_grad)\n",
    "\n",
    "print(\"JAX Gradient_y:\\n\", jax_grad_y)\n",
    "print(\"Finite Difference Gradient_y:\\n\", fd_grad_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule 2: Jax propagates the gradient to all inputs incase of a tie. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [3. 3. 3.]\n",
      "JAX Gradient: [1. 1. 1.]\n",
      "Finite Difference Gradient: [1.001358 1.001358 1.001358]\n",
      "Difference (JAX - Finite Difference): [-0.00135803 -0.00135803 -0.00135803]\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "import numpy as np\n",
    "\n",
    "# Define a simple sum function\n",
    "def sum_function(x):\n",
    "    return jnp.sum(x)\n",
    "\n",
    "# Function to compute finite difference approximation\n",
    "def finite_difference(func, x, h=1e-4):\n",
    "    grad_approx = jnp.zeros_like(x)  # Use jnp for compatibility with JAX arrays\n",
    "    \n",
    "    for i in range(len(x)):  # Loop over each element in the array\n",
    "        x_plus = x.copy()  # Create a copy of the array\n",
    "        x_plus = x_plus.at[i].add(h)  # Perturb one element at a time\n",
    "        grad_approx = grad_approx.at[i].set((func(x_plus) - func(x)) / h)  # Compute finite difference\n",
    "    \n",
    "    return grad_approx\n",
    "\n",
    "# Input array with ties\n",
    "x = jnp.array([3.0, 3.0, 3.0])\n",
    "\n",
    "# Compute the gradient using JAX\n",
    "jax_grad = grad(sum_function)(x)\n",
    "\n",
    "# Compute the finite difference gradient\n",
    "fd_grad = finite_difference(sum_function, x, h=1e-4)\n",
    "\n",
    "# Print results\n",
    "print(\"Input:\", x)\n",
    "print(\"JAX Gradient:\", jax_grad)\n",
    "print(\"Finite Difference Gradient:\", fd_grad)\n",
    "print(\"Difference (JAX - Finite Difference):\", jax_grad - fd_grad)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Test Cases': [Array([3., 2.], dtype=float32),\n",
       "  Array([2., 3.], dtype=float32),\n",
       "  Array([3., 3.], dtype=float32)],\n",
       " 'JAX Gradients': [Array([1., 0.], dtype=float32),\n",
       "  Array([0., 1.], dtype=float32),\n",
       "  Array([0.5, 0.5], dtype=float32)]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "\n",
    "# Define the max function for two variables\n",
    "def max_function(x):\n",
    "    return jnp.max(x)\n",
    "\n",
    "# JAX gradient computation\n",
    "jax_grad = grad(max_function)\n",
    "\n",
    "# Define test cases for (x, y) values\n",
    "test_cases = [\n",
    "    jnp.array([3.0, 2.0]),  # Case where x > y\n",
    "    jnp.array([2.0, 3.0]),  # Case where x < y\n",
    "    jnp.array([3.0, 3.0]),  # Case where x == y\n",
    "]\n",
    "\n",
    "# Calculate gradients for each test case\n",
    "jax_gradients = [jax_grad(case) for case in test_cases]\n",
    "\n",
    "# Store the results\n",
    "results = {\n",
    "    \"Test Cases\": test_cases,\n",
    "    \"JAX Gradients\": jax_gradients\n",
    "}\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Input': Array([3., 1., 2.], dtype=float32),\n",
       "  'JAX Gradient': Array([1., 0., 0.], dtype=float32),\n",
       "  'Finite Difference Gradient': array([0.99897385, 0.        , 0.        ], dtype=float32)},\n",
       " {'Input': Array([1., 2., 2.], dtype=float32),\n",
       "  'JAX Gradient': Array([0. , 0.5, 0.5], dtype=float32),\n",
       "  'Finite Difference Gradient': array([0.        , 0.99897385, 0.99897385], dtype=float32)},\n",
       " {'Input': Array([2., 2., 2.], dtype=float32),\n",
       "  'JAX Gradient': Array([0.33333334, 0.33333334, 0.33333334], dtype=float32),\n",
       "  'Finite Difference Gradient': array([0.99897385, 0.99897385, 0.99897385], dtype=float32)}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def max_function(x):\n",
    "    return jnp.max(x)\n",
    "\n",
    "\n",
    "def finite_difference(func, x, h=1e-4):\n",
    "    grad_approx = np.zeros_like(x)\n",
    "    for i in range(len(x)):  # Iterate over each component\n",
    "        x_plus = x.copy()\n",
    "        x_plus[i] += h  # Perturb one component by h\n",
    "        grad_approx[i] = (func(x_plus) - func(x)) / h\n",
    "    return grad_approx\n",
    "\n",
    "test_cases = [\n",
    "    jnp.array([3.0, 1.0, 2.0]),\n",
    "    jnp.array([1.0, 2.0, 2.0]),\n",
    "    jnp.array([2.0, 2.0, 2.0])\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for x in test_cases:\n",
    "    jax_grad = grad(max_function)(x)\n",
    "\n",
    "    fd_grad = finite_difference(lambda x: np.max(x), np.array(x))\n",
    "\n",
    "\n",
    "    results.append({\n",
    "        \"Input\": x,\n",
    "        \"JAX Gradient\": jax_grad,\n",
    "        \"Finite Difference Gradient\": fd_grad,\n",
    "        \n",
    "    })\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Input': Array([3., 1., 2.], dtype=float32),\n",
       "  'JAX Gradient': Array([0., 1., 0.], dtype=float32),\n",
       "  'Finite Difference Gradient': array([0.       , 1.0001659, 0.       ], dtype=float32)},\n",
       " {'Input': Array([1., 2., 2.], dtype=float32),\n",
       "  'JAX Gradient': Array([1., 0., 0.], dtype=float32),\n",
       "  'Finite Difference Gradient': array([1.0001659, 0.       , 0.       ], dtype=float32)},\n",
       " {'Input': Array([2., 2., 2.], dtype=float32),\n",
       "  'JAX Gradient': Array([0.33333334, 0.33333334, 0.33333334], dtype=float32),\n",
       "  'Finite Difference Gradient': array([0., 0., 0.], dtype=float32)}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "import numpy as np\n",
    "\n",
    "# Test function using max\n",
    "def min_function(x):\n",
    "    return jnp.min(x)\n",
    "\n",
    "# Finite difference gradient function\n",
    "def finite_difference(func, x, h=1e-4):\n",
    "    grad_approx = np.zeros_like(x)\n",
    "    for i in range(len(x)):  # Iterate over each component\n",
    "        x_plus = x.copy()\n",
    "        x_plus[i] += h  # Perturb one component by h\n",
    "        grad_approx[i] = (func(x_plus) - func(x)) / h\n",
    "    return grad_approx\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    jnp.array([3.0, 1.0, 2.0]),\n",
    "    jnp.array([1.0, 2.0, 2.0]),\n",
    "    jnp.array([2.0, 2.0, 2.0])\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for x in test_cases:\n",
    "    # Compute the gradient using JAX\n",
    "    jax_grad = grad(min_function)(x)\n",
    "\n",
    "    # Compute the gradient using finite difference method\n",
    "    fd_grad = finite_difference(lambda x: np.min(x), np.array(x))\n",
    "\n",
    " \n",
    "\n",
    "    results.append({\n",
    "        \"Input\": x,\n",
    "        \"JAX Gradient\": jax_grad,\n",
    "        \"Finite Difference Gradient\": fd_grad,\n",
    "        \n",
    "    })\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Case 1: Input = [3. 1. 2.]\n",
      "JAX Gradient:\n",
      " [1. 0. 0.]\n",
      "Finite Difference Gradient:\n",
      " [0.99897385 0.         0.        ]\n",
      "Difference (JAX - Finite Difference):\n",
      " [0.00102615 0.         0.        ]\n",
      "\n",
      "Test Case 2: Input = [1. 2. 2.]\n",
      "JAX Gradient:\n",
      " [0. 0. 1.]\n",
      "Finite Difference Gradient:\n",
      " [0.         0.99897385 0.99897385]\n",
      "Difference (JAX - Finite Difference):\n",
      " [ 0.         -0.99897385  0.00102615]\n",
      "\n",
      "Test Case 3: Input = [2. 2. 2.]\n",
      "JAX Gradient:\n",
      " [0. 0. 1.]\n",
      "Finite Difference Gradient:\n",
      " [0.99897385 0.99897385 0.99897385]\n",
      "Difference (JAX - Finite Difference):\n",
      " [-0.99897385 -0.99897385  0.00102615]\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sort_function(x):\n",
    "    return jnp.sort(x)[-1]  # Return the largest value after sorting\n",
    "\n",
    "# Finite difference gradient function\n",
    "def finite_difference(func, x, h=1e-4):\n",
    "    grad_approx = np.zeros_like(x)\n",
    "    for i in range(len(x)):\n",
    "        x_plus = x.copy()\n",
    "        x_plus[i] += h\n",
    "        grad_approx[i] = (func(x_plus) - func(x)) / h\n",
    "    return grad_approx\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    jnp.array([3.0, 1.0, 2.0]),  # Case 1: Distinct values\n",
    "    jnp.array([1.0, 2.0, 2.0]),  # Case 2: Tied maximum values\n",
    "    jnp.array([2.0, 2.0, 2.0]),  # Case 3: All values are equal\n",
    "]\n",
    "\n",
    "\n",
    "for idx, x in enumerate(test_cases):\n",
    "    jax_grad = grad(sort_function)(x)\n",
    "\n",
    "    fd_grad = finite_difference(lambda y: np.sort(y)[-1], np.array(x))\n",
    "\n",
    "    print(f\"\\nTest Case {idx + 1}: Input = {x}\")\n",
    "    print(\"JAX Gradient:\\n\", jax_grad)\n",
    "    print(\"Finite Difference Gradient:\\n\", fd_grad)\n",
    "    print(\"Difference (JAX - Finite Difference):\\n\", jax_grad - fd_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clerke's Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX Gradient for max(): [0.33333334 0.33333334 0.33333334]\n",
      "JAX Gradient for sort(): [0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "\n",
    "# Test function using max()\n",
    "def max_function(x):\n",
    "    return jnp.max(x)\n",
    "\n",
    "# Test function using sort()\n",
    "def sort_function(x):\n",
    "    return jnp.sort(x)[-1]\n",
    "\n",
    "# Input array with ties\n",
    "x = jnp.array([3.0, 3.0, 3.0])\n",
    "\n",
    "# Compute the gradients using JAX\n",
    "jax_grad_max = grad(max_function)(x)\n",
    "jax_grad_sort = grad(sort_function)(x)\n",
    "\n",
    "# Print results\n",
    "print(\"JAX Gradient for max():\", jax_grad_max)\n",
    "print(\"JAX Gradient for sort():\", jax_grad_sort)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical derivative of max and sort functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Function to illustrate max and sort subdifferentials\n",
    "x = np.linspace(-1, 3, 400)\n",
    "y_max = np.maximum(x, 1)  # Max function example: max(x, 1)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Max function plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, y_max, label=r\"$f(x) = \\max(x, 1)$\", color=\"blue\")\n",
    "plt.axvline(1, color=\"red\", linestyle=\"--\", label=r\"$x=1$ (point of non-differentiability)\")\n",
    "plt.title(\"Max Function\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.legend()\n",
    "\n",
    "# Sort function plot (illustrating the largest value)\n",
    "x_values = np.array([2, 3, 1, 4, 0])\n",
    "sorted_values = np.sort(x_values)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.step(np.arange(len(sorted_values)), sorted_values, where=\"mid\", label=\"Sorted values\", color=\"green\")\n",
    "plt.scatter(np.arange(len(x_values)), sorted_values, color=\"black\")\n",
    "plt.title(\"Sort Function\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Sorted Value\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "import numpy as np\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sort() comparing the 3 methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Case 1: Input = [3. 1. 2.]\n",
      "JAX Gradient:\n",
      " [1. 0. 0.]\n",
      "Finite Difference Gradient:\n",
      " [0.99897385 0.         0.        ]\n",
      "Symbolic Gradient:\n",
      " [1. 0. 0.]\n",
      "\n",
      "Test Case 2: Input = [1. 2. 2.]\n",
      "JAX Gradient:\n",
      " [0. 0. 1.]\n",
      "Finite Difference Gradient:\n",
      " [0.         0.99897385 0.99897385]\n",
      "Symbolic Gradient:\n",
      " [0.  0.5 0.5]\n",
      "\n",
      "Test Case 3: Input = [2. 2. 2.]\n",
      "JAX Gradient:\n",
      " [0. 0. 1.]\n",
      "Finite Difference Gradient:\n",
      " [0.99897385 0.99897385 0.99897385]\n",
      "Symbolic Gradient:\n",
      " [0.5 0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def sort_function(x):\n",
    "    return jnp.sort(x)[-1]  # Return the largest value after sorting\n",
    "\n",
    "def finite_difference(func, x, h=1e-4):\n",
    "    grad_approx = np.zeros_like(x)\n",
    "    for i in range(len(x)):  # Iterate over each component\n",
    "        x_plus = x.copy()\n",
    "        x_plus[i] += h  # Perturb one component by h\n",
    "        grad_approx[i] = (func(x_plus) - func(x)) / h\n",
    "    return grad_approx\n",
    "\n",
    "def symbolic_gradient_sort(x):\n",
    "    x_symbols = sp.symbols(f\"x0:{len(x)}\")\n",
    "    max_val = sp.Max(*x_symbols) \n",
    "    gradients = [sp.diff(max_val, xi) for xi in x_symbols]\n",
    "    subs = {x_symbols[i]: x[i] for i in range(len(x))}\n",
    "    grad_values = [float(grad.evalf(subs=subs)) for grad in gradients]\n",
    "\n",
    "    return np.array(grad_values)\n",
    "\n",
    "\n",
    "\n",
    "test_cases = [\n",
    "    jnp.array([3.0, 1.0, 2.0]),  # Case 1: Distinct values\n",
    "    jnp.array([1.0, 2.0, 2.0]),  # Case 2: Tied maximum values\n",
    "    jnp.array([2.0, 2.0, 2.0])   # Case 3: All values are equal\n",
    "]\n",
    "\n",
    "for idx, x in enumerate(test_cases):\n",
    "    jax_grad = grad(sort_function)(x)\n",
    "    \n",
    "    fd_grad = finite_difference(lambda y: np.sort(y)[-1], np.array(x))\n",
    "    \n",
    "    sym_grad = symbolic_gradient_sort(np.array(x))\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nTest Case {idx + 1}: Input = {x}\")\n",
    "    print(\"JAX Gradient:\\n\", jax_grad)\n",
    "    print(\"Finite Difference Gradient:\\n\", fd_grad)\n",
    "    print(\"Symbolic Gradient:\\n\", sym_grad)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# min() comparing the 3 functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1: Input = [3. 1. 2.]\n",
      "JAX Gradient: [0. 1. 0.]\n",
      "Finite Difference Gradient: [0.        1.0001659 0.       ]\n",
      "Symbolic Gradient: [0. 1. 0.]\n",
      "Difference (JAX - Finite Difference): [ 0.         -0.00016594  0.        ]\n",
      "Difference (JAX - Symbolic): [0. 0. 0.]\n",
      "\n",
      "Test Case 2: Input = [1. 2. 2.]\n",
      "JAX Gradient: [1. 0. 0.]\n",
      "Finite Difference Gradient: [1.0001659 0.        0.       ]\n",
      "Symbolic Gradient: [1. 0. 0.]\n",
      "Difference (JAX - Finite Difference): [-0.00016594  0.          0.        ]\n",
      "Difference (JAX - Symbolic): [0. 0. 0.]\n",
      "\n",
      "Test Case 3: Input = [2. 2. 2.]\n",
      "JAX Gradient: [0.33333334 0.33333334 0.33333334]\n",
      "Finite Difference Gradient: [0. 0. 0.]\n",
      "Symbolic Gradient: [0.5 0.5 0.5]\n",
      "Difference (JAX - Finite Difference): [0.33333334 0.33333334 0.33333334]\n",
      "Difference (JAX - Symbolic): [-0.16666666 -0.16666666 -0.16666666]\n",
      "\n",
      "Test Case 1: Input = [3. 1. 2.]\n",
      "JAX Gradient: [0. 1. 0.]\n",
      "Finite Difference Gradient: [0.        1.0001659 0.       ]\n",
      "Symbolic Gradient: [0. 1. 0.]\n",
      "Difference (JAX - Finite Difference): [ 0.         -0.00016594  0.        ]\n",
      "Difference (JAX - Symbolic): [0. 0. 0.]\n",
      "\n",
      "Test Case 2: Input = [1. 2. 2.]\n",
      "JAX Gradient: [1. 0. 0.]\n",
      "Finite Difference Gradient: [1.0001659 0.        0.       ]\n",
      "Symbolic Gradient: [1. 0. 0.]\n",
      "Difference (JAX - Finite Difference): [-0.00016594  0.          0.        ]\n",
      "Difference (JAX - Symbolic): [0. 0. 0.]\n",
      "\n",
      "Test Case 3: Input = [2. 2. 2.]\n",
      "JAX Gradient: [0.33333334 0.33333334 0.33333334]\n",
      "Finite Difference Gradient: [0. 0. 0.]\n",
      "Symbolic Gradient: [0.5 0.5 0.5]\n",
      "Difference (JAX - Finite Difference): [0.33333334 0.33333334 0.33333334]\n",
      "Difference (JAX - Symbolic): [-0.16666666 -0.16666666 -0.16666666]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "import numpy as np\n",
    "\n",
    "# Test function using min\n",
    "def min_function(x):\n",
    "    return jnp.min(x)\n",
    "\n",
    "# Finite difference gradient function\n",
    "def finite_difference(func, x, h=1e-4):\n",
    "    grad_approx = np.zeros_like(x)\n",
    "    for i in range(len(x)):  # Iterate over each component\n",
    "        x_plus = x.copy()\n",
    "        x_plus[i] += h  # Perturb one component by h\n",
    "        grad_approx[i] = (func(x_plus) - func(x)) / h\n",
    "    return grad_approx\n",
    "\n",
    "\n",
    "import sympy as sp\n",
    "import numpy as np\n",
    "\n",
    "def symbolic_gradient_min(x):\n",
    "    \"\"\"\n",
    "    Compute symbolic gradients for the min value extracted from an array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        Input array for which to compute symbolic gradients.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Symbolic gradient values evaluated for the input array.\n",
    "    \"\"\"\n",
    "    # Define symbolic variables\n",
    "    x_symbols = sp.symbols(f\"x0:{len(x)}\")\n",
    "\n",
    "    # Define the symbolic min function\n",
    "    min_val = sp.Min(*x_symbols)  # Use sp.Min to represent the minimum value symbolically\n",
    "\n",
    "    # Compute symbolic gradients\n",
    "    gradients = [sp.diff(min_val, xi) for xi in x_symbols]\n",
    "\n",
    "    # Substitute the input values and evaluate the gradients\n",
    "    subs = {x_symbols[i]: x[i] for i in range(len(x))}\n",
    "    grad_values = [float(grad.evalf(subs=subs)) for grad in gradients]\n",
    "\n",
    "    return np.array(grad_values)\n",
    "\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    jnp.array([3.0, 1.0, 2.0]),  # Distinct values\n",
    "    jnp.array([1.0, 2.0, 2.0]),  # Tied minimum values\n",
    "    jnp.array([2.0, 2.0, 2.0])   # All values equal\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for x in test_cases:\n",
    "    # Compute the gradient using JAX\n",
    "    jax_grad = grad(min_function)(x)\n",
    "\n",
    "    # Compute the gradient using finite difference method\n",
    "    fd_grad = finite_difference(lambda x: np.min(x), np.array(x))\n",
    "\n",
    "    # Compute the gradient using symbolic differentiation\n",
    "    sym_grad = symbolic_gradient_min(np.array(x))\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        \"Input\": x,\n",
    "        \"JAX Gradient\": jax_grad,\n",
    "        \"Finite Difference Gradient\": fd_grad,\n",
    "        \"Symbolic Gradient\": sym_grad,\n",
    "    })\n",
    "\n",
    "# Print results\n",
    "for idx, result in enumerate(results):\n",
    "    print(f\"Test Case {idx + 1}: Input = {result['Input']}\")\n",
    "    print(\"JAX Gradient:\", result[\"JAX Gradient\"])\n",
    "    print(\"Finite Difference Gradient:\", result[\"Finite Difference Gradient\"])\n",
    "    print(\"Symbolic Gradient:\", result[\"Symbolic Gradient\"])\n",
    "    print(\"Difference (JAX - Finite Difference):\", result[\"JAX Gradient\"] - result[\"Finite Difference Gradient\"])\n",
    "    print(\"Difference (JAX - Symbolic):\", result[\"JAX Gradient\"] - result[\"Symbolic Gradient\"])\n",
    "    print()\n",
    "\n",
    "\n",
    "# Test function using min\n",
    "def min_function(x):\n",
    "    return jnp.min(x)\n",
    "\n",
    "# Finite difference gradient function\n",
    "def finite_difference(func, x, h=1e-4):\n",
    "    grad_approx = np.zeros_like(x)\n",
    "    for i in range(len(x)):  # Iterate over each component\n",
    "        x_plus = x.copy()\n",
    "        x_plus[i] += h  # Perturb one component by h\n",
    "        grad_approx[i] = (func(x_plus) - func(x)) / h\n",
    "    return grad_approx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    jnp.array([3.0, 1.0, 2.0]),  # Distinct values\n",
    "    jnp.array([1.0, 2.0, 2.0]),  # Tied minimum values\n",
    "    jnp.array([2.0, 2.0, 2.0])   # All values equal\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for x in test_cases:\n",
    "    # Compute the gradient using JAX\n",
    "    jax_grad = grad(min_function)(x)\n",
    "\n",
    "    # Compute the gradient using finite difference method\n",
    "    fd_grad = finite_difference(lambda x: np.min(x), np.array(x))\n",
    "\n",
    "    # Compute the gradient using symbolic differentiation\n",
    "    sym_grad = symbolic_gradient_min(np.array(x))\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        \"Input\": x,\n",
    "        \"JAX Gradient\": jax_grad,\n",
    "        \"Finite Difference Gradient\": fd_grad,\n",
    "        \"Symbolic Gradient\": sym_grad,\n",
    "    })\n",
    "\n",
    "# Print results\n",
    "for idx, result in enumerate(results):\n",
    "    print(f\"Test Case {idx + 1}: Input = {result['Input']}\")\n",
    "    print(\"JAX Gradient:\", result[\"JAX Gradient\"])\n",
    "    print(\"Finite Difference Gradient:\", result[\"Finite Difference Gradient\"])\n",
    "    print(\"Symbolic Gradient:\", result[\"Symbolic Gradient\"])\n",
    "    print(\"Difference (JAX - Finite Difference):\", result[\"JAX Gradient\"] - result[\"Finite Difference Gradient\"])\n",
    "    print(\"Difference (JAX - Symbolic):\", result[\"JAX Gradient\"] - result[\"Symbolic Gradient\"])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1: Input = [3. 1. 2.]\n",
      "JAX Gradient: [1. 0. 0.]\n",
      "Finite Difference Gradient: [0.99897385 0.         0.        ]\n",
      "Symbolic Gradient: [1. 0. 0.]\n",
      "Difference (JAX - Finite Difference): [0.00102615 0.         0.        ]\n",
      "Difference (JAX - Symbolic): [0. 0. 0.]\n",
      "\n",
      "Test Case 2: Input = [1. 2. 2.]\n",
      "JAX Gradient: [0.  0.5 0.5]\n",
      "Finite Difference Gradient: [0.         0.99897385 0.99897385]\n",
      "Symbolic Gradient: [0.  0.5 0.5]\n",
      "Difference (JAX - Finite Difference): [ 0.         -0.49897385 -0.49897385]\n",
      "Difference (JAX - Symbolic): [0. 0. 0.]\n",
      "\n",
      "Test Case 3: Input = [2. 2. 2.]\n",
      "JAX Gradient: [0.33333334 0.33333334 0.33333334]\n",
      "Finite Difference Gradient: [0.99897385 0.99897385 0.99897385]\n",
      "Symbolic Gradient: [0.5 0.5 0.5]\n",
      "Difference (JAX - Finite Difference): [-0.6656405 -0.6656405 -0.6656405]\n",
      "Difference (JAX - Symbolic): [-0.16666666 -0.16666666 -0.16666666]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "import numpy as np\n",
    "\n",
    "# Test function using max\n",
    "def max_function(x):\n",
    "    return jnp.max(x)\n",
    "\n",
    "# Finite difference gradient function\n",
    "def finite_difference(func, x, h=1e-4):\n",
    "    grad_approx = np.zeros_like(x)\n",
    "    for i in range(len(x)):  # Iterate over each component\n",
    "        x_plus = x.copy()\n",
    "        x_plus[i] += h  # Perturb one component by h\n",
    "        grad_approx[i] = (func(x_plus) - func(x)) / h\n",
    "    return grad_approx\n",
    "\n",
    "\n",
    "import sympy as sp\n",
    "\n",
    "def symbolic_gradient_max(x):\n",
    "    \"\"\"\n",
    "    Compute symbolic gradients for the max value extracted from an array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        Input array for which to compute symbolic gradients.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Symbolic gradient values evaluated for the input array.\n",
    "    \"\"\"\n",
    "    # Define symbolic variables\n",
    "    x_symbols = sp.symbols(f\"x0:{len(x)}\")\n",
    "\n",
    "    # Define the symbolic max function\n",
    "    max_val = sp.Max(*x_symbols)  # Use sp.Max to represent the maximum value symbolically\n",
    "\n",
    "    # Compute symbolic gradients\n",
    "    gradients = [sp.diff(max_val, xi) for xi in x_symbols]\n",
    "\n",
    "    # Substitute the input values and evaluate the gradients\n",
    "    subs = {x_symbols[i]: x[i] for i in range(len(x))}\n",
    "    grad_values = [float(grad.evalf(subs=subs)) for grad in gradients]\n",
    "\n",
    "    return np.array(grad_values)\n",
    "\n",
    "\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    jnp.array([3.0, 1.0, 2.0]),  # Distinct values\n",
    "    jnp.array([1.0, 2.0, 2.0]),  # Tied maximum values\n",
    "    jnp.array([2.0, 2.0, 2.0])   # All values equal\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for x in test_cases:\n",
    "    # Compute the gradient using JAX\n",
    "    jax_grad = grad(max_function)(x)\n",
    "\n",
    "    # Compute the gradient using finite difference method\n",
    "    fd_grad = finite_difference(lambda x: np.max(x), np.array(x))\n",
    "\n",
    "    # Compute the gradient using symbolic differentiation\n",
    "    sym_grad = symbolic_gradient_max(np.array(x))\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        \"Input\": x,\n",
    "        \"JAX Gradient\": jax_grad,\n",
    "        \"Finite Difference Gradient\": fd_grad,\n",
    "        \"Symbolic Gradient\": sym_grad,\n",
    "    })\n",
    "\n",
    "# Print results\n",
    "for idx, result in enumerate(results):\n",
    "    print(f\"Test Case {idx + 1}: Input = {result['Input']}\")\n",
    "    print(\"JAX Gradient:\", result[\"JAX Gradient\"])\n",
    "    print(\"Finite Difference Gradient:\", result[\"Finite Difference Gradient\"])\n",
    "    print(\"Symbolic Gradient:\", result[\"Symbolic Gradient\"])\n",
    "    print(\"Difference (JAX - Finite Difference):\", result[\"JAX Gradient\"] - result[\"Finite Difference Gradient\"])\n",
    "    print(\"Difference (JAX - Symbolic):\", result[\"JAX Gradient\"] - result[\"Symbolic Gradient\"])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the discontinuity caused by the tied maximum values (\n",
    "2.0\n",
    "2.0 at indices \n",
    "1\n",
    "1 and \n",
    "2\n",
    "2), JAX and symbolic differentiation correctly compute a convex combination of the gradients, assigning equal weight (\n",
    "0.5\n",
    "0.5) to both indices. The finite difference method fails to capture this behavior due to its reliance on perturbations, which do not adequately represent the non-smooth transition between tied maxima.  \n",
    "\n",
    "For completely tied values, JAX computes a uniform convex combination of gradients, dividing responsibility equally among all indices. Symbolic differentiation provides an equal gradient of \n",
    "0.5\n",
    "0.5, which aligns with a different convention but is consistent with the hyperplane condition of subgradients.\n",
    "Finite difference again fails to account for the discontinuity, leading to overestimated gradients.  \n",
    "\n",
    "JAX computes subgradients for the max() function based on Clarke's generalized gradient. The subgradient at a discontinuity is the convex hull of the gradients from all possible smooth regions near the discontinuity. This explains the uniform distribution observed in Test Case 3:\n",
    "\n",
    "JAX Gradient: \n",
    "[\n",
    "0.3333\n",
    ",\n",
    "0.3333\n",
    ",\n",
    "0.3333\n",
    "]\n",
    ".\n",
    "JAX Gradient: [0.3333,0.3333,0.3333]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
